{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2kLrOh-bpGy"
   },
   "source": [
    "# Iris Flower - Batch Prediction\n",
    "\n",
    "\n",
    "In this notebook we will, \n",
    "\n",
    "1. Load the batch inference data that arrived in the last 24 hours\n",
    "2. Predict the first Iris Flower found in the batch\n",
    "3. Write the ouput png of the Iris flower predicted, to be displayed in Github Pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xRtpj-psbpG8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/23699\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "import joblib\n",
    "\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Downloading file ... "
     ]
    }
   ],
   "source": [
    "mr = project.get_model_registry()\n",
    "model = mr.get_model(\"airis\", version=1)\n",
    "model_dir = model.download()\n",
    "model = joblib.load(model_dir + \"/airis_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are downloading the 'raw' iris data. We explicitly do not want transformed data, reading for training. \n",
    "\n",
    "So, let's download the iris dataset, and preview some rows. \n",
    "\n",
    "Note, that it is 'tabular data'. There are 5 columns: 4 of them are \"features\", and the \"variety\" column is the **target** (what we are trying to predict using the 4 feature values in the target's row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nRmFM7vcbpHA",
    "outputId": "d920d168-9818-40c5-c292-4cf0afcbbcfd"
   },
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(name=\"airis\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do some **Batch Inference**. \n",
    "\n",
    "We will read all the input features that have arrived in the last 24 hours, and score them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uHuAD3ttP8Ep"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No training dataset version was provided to initialise batch scoring . Defaulting to version 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-09 00:08:33,306 INFO: USE `rec_featurestore`\n",
      "2023-06-09 00:08:34,226 INFO: SELECT `fg0`.`sepal_length` `sepal_length`, `fg0`.`sepal_width` `sepal_width`, `fg0`.`petal_length` `petal_length`, `fg0`.`petal_width` `petal_width`\n",
      "FROM `rec_featurestore`.`airis_1` `fg0`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Setosa', 'Virginica', 'Versicolor', 'Virginica',\n",
       "       'Setosa', 'Virginica', 'Versicolor', 'Versicolor', 'Versicolor',\n",
       "       'Virginica', 'Virginica', 'Virginica', 'Virginica', 'Virginica',\n",
       "       'Virginica', 'Versicolor', 'Setosa', 'Versicolor', 'Versicolor',\n",
       "       'Versicolor', 'Versicolor', 'Versicolor', 'Setosa', 'Virginica',\n",
       "       'Setosa', 'Setosa', 'Versicolor', 'Setosa', 'Setosa', 'Versicolor',\n",
       "       'Versicolor', 'Virginica', 'Virginica', 'Setosa', 'Setosa',\n",
       "       'Virginica', 'Setosa', 'Versicolor', 'Setosa', 'Virginica',\n",
       "       'Versicolor', 'Versicolor', 'Setosa', 'Versicolor', 'Versicolor',\n",
       "       'Virginica', 'Versicolor', 'Versicolor', 'Versicolor',\n",
       "       'Versicolor', 'Virginica', 'Versicolor', 'Virginica', 'Versicolor',\n",
       "       'Setosa', 'Versicolor', 'Virginica', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Virginica', 'Setosa', 'Setosa', 'Versicolor', 'Versicolor',\n",
       "       'Virginica', 'Versicolor', 'Versicolor', 'Virginica', 'Versicolor',\n",
       "       'Versicolor', 'Setosa', 'Virginica', 'Virginica', 'Virginica',\n",
       "       'Versicolor', 'Virginica', 'Virginica', 'Virginica', 'Versicolor',\n",
       "       'Virginica', 'Versicolor', 'Virginica', 'Virginica', 'Setosa',\n",
       "       'Versicolor', 'Virginica', 'Versicolor', 'Setosa', 'Versicolor',\n",
       "       'Versicolor', 'Virginica', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Versicolor', 'Virginica', 'Setosa', 'Virginica', 'Setosa',\n",
       "       'Setosa', 'Versicolor', 'Virginica', 'Setosa', 'Versicolor',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Virginica', 'Setosa', 'Virginica',\n",
       "       'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Versicolor', 'Setosa', 'Setosa', 'Setosa', 'Versicolor',\n",
       "       'Virginica', 'Versicolor', 'Setosa', 'Virginica', 'Setosa',\n",
       "       'Versicolor', 'Setosa', 'Setosa', 'Virginica', 'Virginica',\n",
       "       'Setosa', 'Versicolor', 'Setosa', 'Versicolor', 'Virginica',\n",
       "       'Virginica', 'Versicolor', 'Versicolor', 'Virginica', 'Versicolor',\n",
       "       'Virginica', 'Versicolor', 'Versicolor', 'Setosa', 'Versicolor',\n",
       "       'Setosa', 'Versicolor'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from PIL import Image\n",
    "\n",
    "batch_data = feature_view.get_batch_data()\n",
    "\n",
    "y_pred = model.predict(batch_data)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.610476</td>\n",
       "      <td>3.21236</td>\n",
       "      <td>1.905128</td>\n",
       "      <td>0.442144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.900000</td>\n",
       "      <td>3.60000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.700000</td>\n",
       "      <td>2.50000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.700000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.700000</td>\n",
       "      <td>3.30000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>2.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>2.70000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.40000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.20000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>3.30000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "0        4.610476      3.21236      1.905128     0.442144\n",
       "1        4.900000      3.60000      1.400000     0.100000\n",
       "2        5.700000      2.50000      5.000000     2.000000\n",
       "3        5.700000      3.00000      4.200000     1.200000\n",
       "4        6.700000      3.30000      5.700000     2.100000\n",
       "..            ...          ...           ...          ...\n",
       "145      5.800000      2.70000      3.900000     1.200000\n",
       "146      5.000000      3.40000      1.600000     0.400000\n",
       "147      6.200000      2.20000      4.500000     1.500000\n",
       "148      5.000000      3.00000      1.600000     0.200000\n",
       "149      6.300000      3.30000      4.700000     1.600000\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch prediction output is the last entry in the batch - it is output as a file 'latest_iris.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower = y_pred[y_pred.size-1]\n",
    "flower_img = \"assets/\" + flower + \".png\"\n",
    "img = Image.open(flower_img)            \n",
    "\n",
    "img.save(\"../../assets/latest_iris.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-09 00:08:49,454 INFO: USE `rec_featurestore`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot access feature store 'rec_featurestore'. Please check if your project has the access right. It is possible to request access from data owners of 'rec_featurestore'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\hsfs\\engine\\python.py:627\u001b[0m, in \u001b[0;36mEngine._create_hive_connection\u001b[1;34m(self, feature_store)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9085\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# database needs to be set every time, 'default' doesn't work in pyhive\u001b[39;49;00m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCERTIFICATES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruststore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_jks_trust_store_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeystore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_jks_key_store_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeystore_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cert_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (TTransportException, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyhive\\hive.py:247\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, host, port, username, database, auth, configuration, kerberos_service_name, password, keystore, truststore, keystore_password, thrift_transport)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor()) \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[1;32m--> 247\u001b[0m         \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUSE `\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m`\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyhive\\hive.py:408\u001b[0m, in \u001b[0;36mCursor.execute\u001b[1;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mExecuteStatement(req)\n\u001b[1;32m--> 408\u001b[0m \u001b[43m_check_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operationHandle \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39moperationHandle\n",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyhive\\hive.py:538\u001b[0m, in \u001b[0;36m_check_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mstatusCode \u001b[38;5;241m!=\u001b[39m ttypes\u001b[38;5;241m.\u001b[39mTStatusCode\u001b[38;5;241m.\u001b[39mSUCCESS_STATUS:\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError(response)\n",
      "\u001b[1;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:MetaException(message:Got exception: javax.net.ssl.SSLException DestHost:destPort ip-172-16-4-235.us-east-2.compute.internal:8020 , LocalHost:localPort ip-172-16-4-138.us-east-2.compute.internal/172.16.4.138:0. Failed on local exception: javax.net.ssl.SSLException: readHandshakeRecord)):28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:337', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:232', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:269', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:253', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'sun.reflect.GeneratedMethodAccessor212:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1821', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy53:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:712', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.metadata.HiveException:MetaException(message:MetaException(message:Got exception: javax.net.ssl.SSLException DestHost:destPort ip-172-16-4-235.us-east-2.compute.internal:8020 , LocalHost:localPort ip-172-16-4-138.us-east-2.compute.internal/172.16.4.138:0. Failed on local exception: javax.net.ssl.SSLException: readHandshakeRecord)):39:12', 'org.apache.hadoop.hive.ql.metadata.Hive:getDatabase:Hive.java:1626', 'org.apache.hadoop.hive.ql.metadata.Hive:databaseExists:Hive.java:1611', 'org.apache.hadoop.hive.ql.exec.DDLTask:switchDatabase:DDLTask.java:4844', 'org.apache.hadoop.hive.ql.exec.DDLTask:execute:DDLTask.java:412', 'org.apache.hadoop.hive.ql.exec.Task:executeTask:Task.java:210', 'org.apache.hadoop.hive.ql.exec.TaskRunner:runSequential:TaskRunner.java:97', 'org.apache.hadoop.hive.ql.Driver:launchTask:Driver.java:2490', 'org.apache.hadoop.hive.ql.Driver:execute:Driver.java:2146', 'org.apache.hadoop.hive.ql.Driver:runInternal:Driver.java:1823', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1566', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1560', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:run:ReExecDriver.java:162', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:230', '*org.apache.hadoop.hive.metastore.api.MetaException:MetaException(message:Got exception: javax.net.ssl.SSLException DestHost:destPort ip-172-16-4-235.us-east-2.compute.internal:8020 , LocalHost:localPort ip-172-16-4-138.us-east-2.compute.internal/172.16.4.138:0. Failed on local exception: javax.net.ssl.SSLException: readHandshakeRecord):57:18', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result$get_database_resultStandardScheme:read:ThriftHiveMetastore.java:39333', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result$get_database_resultStandardScheme:read:ThriftHiveMetastore.java:39301', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result:read:ThriftHiveMetastore.java:39232', 'org.apache.thrift.TServiceClient:receiveBase:TServiceClient.java:86', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:recv_get_database:ThriftHiveMetastore.java:1106', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:get_database:ThriftHiveMetastore.java:1093', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:getDatabase:HiveMetaStoreClient.java:1712', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:getDatabase:HiveMetaStoreClient.java:1707', 'sun.reflect.GeneratedMethodAccessor180:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingMetaStoreClient:invoke:RetryingMetaStoreClient.java:212', 'com.sun.proxy.$Proxy34:getDatabase::-1', 'sun.reflect.GeneratedMethodAccessor180:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler:invoke:HiveMetaStoreClient.java:2975', 'com.sun.proxy.$Proxy34:getDatabase::-1', 'org.apache.hadoop.hive.ql.metadata.Hive:getDatabase:Hive.java:1622'], sqlState='08S01', errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:MetaException(message:Got exception: javax.net.ssl.SSLException DestHost:destPort ip-172-16-4-235.us-east-2.compute.internal:8020 , LocalHost:localPort ip-172-16-4-138.us-east-2.compute.internal/172.16.4.138:0. Failed on local exception: javax.net.ssl.SSLException: readHandshakeRecord))'), operationHandle=None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m iris_fg \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mget_feature_group(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairis\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43miris_fg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df\n",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\hsfs\\feature_group.py:845\u001b[0m, in \u001b[0;36mFeatureGroup.read\u001b[1;34m(self, wallclock_time, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_all()\n\u001b[0;32m    837\u001b[0m         \u001b[38;5;241m.\u001b[39mas_of(wallclock_time)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43monline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\hsfs\\constructor\\query.py:108\u001b[0m, in \u001b[0;36mQuery.read\u001b[1;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read the specified query into a DataFrame.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mIt is possible to specify the storage (online/offline) to read from and the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    `DataFrame`: DataFrame depending on the chosen type.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m sql_query, online_conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_read(online, read_options)\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_store_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43monline_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\hsfs\\engine\\python.py:84\u001b[0m, in \u001b[0;36mEngine.sql\u001b[1;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sql_query, feature_store, online_conn, dataframe_type, read_options):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_offline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdbc(sql_query, online_conn, dataframe_type, read_options)\n",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\hsfs\\engine\\python.py:89\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[1;34m(self, sql_query, feature_store, dataframe_type)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sql_offline\u001b[39m(\u001b[38;5;28mself\u001b[39m, sql_query, feature_store, dataframe_type):\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_hive_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_store\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m hive_conn:\n\u001b[0;32m     90\u001b[0m         result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(sql_query, hive_conn)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_dataframe_type(result_df, dataframe_type)\n",
      "File \u001b[1;32mc:\\users\\fadil\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\hsfs\\engine\\python.py:644\u001b[0m, in \u001b[0;36mEngine._create_hive_connection\u001b[1;34m(self, feature_store)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mstatusCode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access feature store \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_store\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please check if your project has the access right.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible to request access from data owners of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_store\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot access feature store 'rec_featurestore'. Please check if your project has the access right. It is possible to request access from data owners of 'rec_featurestore'."
     ]
    }
   ],
   "source": [
    "iris_fg = fs.get_feature_group(name=\"airis\", version=1)\n",
    "df = iris_fg.read()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df.iloc[-1][\"variety\"]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_flower = \"assets/\" + label + \".png\"\n",
    "\n",
    "img = Image.open(label_flower)            \n",
    "\n",
    "img.save(\"../../assets/actual_iris.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "monitor_fg = fs.get_or_create_feature_group(name=\"airis_predictions\",\n",
    "                                  version=1,\n",
    "                                  primary_key=[\"datetime\"],\n",
    "                                  description=\"Iris flower Prediction/Outcome Monitoring\"\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "data = {\n",
    "    'prediction': [flower],\n",
    "    'label': [label],\n",
    "    'datetime': [now],\n",
    "}\n",
    "monitor_df = pd.DataFrame(data)\n",
    "monitor_fg.insert(monitor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = monitor_fg.read()\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe_image as dfi\n",
    "\n",
    "df_recent = history_df.tail(5)\n",
    " \n",
    "# If you exclude this image, you may have the same iris_latest.png and iris_actual.png files\n",
    "# If no files have changed, the GH-action 'git commit/push' stage fails, failing your GH action (last step)\n",
    "# This image, however, is always new, ensuring git commit/push will succeed.\n",
    "dfi.export(df_recent, '../../assets/df_recent.png', table_conversion = 'matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = history_df[['prediction']]\n",
    "labels = history_df[['label']]\n",
    "\n",
    "results = confusion_matrix(labels, predictions)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "# Only create the confusion matrix when our iris_predictions feature group has examples of all 3 iris flowers\n",
    "if results.shape == (3,3):\n",
    "\n",
    "    df_cm = pd.DataFrame(results, ['True Setosa', 'True Versicolor', 'True Virginica'],\n",
    "                         ['Pred Setosa', 'Pred Versicolor', 'Pred Virginica'])\n",
    "\n",
    "    cm = sns.heatmap(df_cm, annot=True)\n",
    "\n",
    "    fig = cm.get_figure()\n",
    "    fig.savefig(\"../../assets/confusion_matrix.png\") \n",
    "    df_cm\n",
    "else:\n",
    "    print(\"Run the batch inference pipeline more times until you get 3 different iris flowers\")    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
